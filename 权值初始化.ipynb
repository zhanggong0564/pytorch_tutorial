{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,neural_num,layers):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear = nn.ModuleList([nn.Linear(neural_num,neural_num,bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "    def forward(self,x):\n",
    "        for (i,linear) in enumerate(self.linear):\n",
    "            x =linear(x)\n",
    "        return x\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_nums = 100\n",
    "neurAL_nums = 256\n",
    "batch_size= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(neural_num=neurAL_nums,layers=layer_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(batch_size,neurAL_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现数据超出了可表示的范围（要么非常大，要么非常小）  \n",
    "现在我们观察一下数据什么时候出现了nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,neural_num,layers):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear = nn.ModuleList([nn.Linear(neural_num,neural_num,bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "    def forward(self,x):\n",
    "        for (i,linear) in enumerate(self.linear):\n",
    "            x =linear(x)\n",
    "            print(\"layers:{}, std:{}\".format(i,x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "        return x\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = MLP(neurAL_nums,layer_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:16.083585739135742\n",
      "layers:1, std:261.1745910644531\n",
      "layers:2, std:4165.44287109375\n",
      "layers:3, std:67348.359375\n",
      "layers:4, std:1060944.75\n",
      "layers:5, std:16473645.0\n",
      "layers:6, std:268507744.0\n",
      "layers:7, std:4413952512.0\n",
      "layers:8, std:68022620160.0\n",
      "layers:9, std:1075242074112.0\n",
      "layers:10, std:17344401244160.0\n",
      "layers:11, std:280017523179520.0\n",
      "layers:12, std:4513735213318144.0\n",
      "layers:13, std:7.149678727318733e+16\n",
      "layers:14, std:1.1240694261408072e+18\n",
      "layers:15, std:1.816435760185947e+19\n",
      "layers:16, std:2.926854037534136e+20\n",
      "layers:17, std:4.750459374395229e+21\n",
      "layers:18, std:7.5197882000499345e+22\n",
      "layers:19, std:1.1979136178057828e+24\n",
      "layers:20, std:1.927403442855524e+25\n",
      "layers:21, std:3.0146831310132793e+26\n",
      "layers:22, std:4.7524355611535073e+27\n",
      "layers:23, std:7.439961362102737e+28\n",
      "layers:24, std:1.1938434877627082e+30\n",
      "layers:25, std:1.8699478403222863e+31\n",
      "layers:26, std:3.011275605592508e+32\n",
      "layers:27, std:4.7634230544147416e+33\n",
      "layers:28, std:7.726996420132194e+34\n",
      "layers:29, std:1.2105679767873022e+36\n",
      "layers:30, std:1.9226113120348893e+37\n",
      "layers:31, std:nan\n",
      "output is nan in 31 layers\n"
     ]
    }
   ],
   "source": [
    "output = net2(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[        inf, -2.2689e+38,         inf,  ...,  7.6598e+37,\n",
      "          6.8875e+37, -4.8236e+36],\n",
      "        [-1.0201e+38, -1.2558e+38, -2.1341e+38,  ...,  9.9613e+37,\n",
      "          6.0173e+37,        -inf],\n",
      "        [-2.2543e+38,        -inf, -1.0540e+38,  ...,         inf,\n",
      "          7.9858e+37, -1.6063e+37],\n",
      "        ...,\n",
      "        [ 2.1819e+38,         inf,  2.3744e+38,  ...,        -inf,\n",
      "          9.7533e+37,  9.7264e+37],\n",
      "        [-9.7689e+37, -2.3595e+38,         inf,  ...,         inf,\n",
      "          1.3836e+38,        -inf],\n",
      "        [       -inf, -1.1059e+38,        -inf,  ...,         inf,\n",
      "         -1.5182e+38, -1.5844e+38]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到网络权值是一个逐渐变大的过程，在31层就变到了 无法表示的了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D(H_1) = sum(1-n) (D(x_i)*D(w_{1i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要使得D不变必须D(w) = 1/n =>std(w) = (1/n)^1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,neural_num,layers):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear = nn.ModuleList([nn.Linear(neural_num,neural_num,bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "    def forward(self,x):\n",
    "        for (i,linear) in enumerate(self.linear):\n",
    "            x =linear(x)\n",
    "            \n",
    "            print(\"layers:{}, std:{}\".format(i,x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "        return x\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.normal_(m.weight.data,std=np.sqrt(1/self.neural_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = MLP(neurAL_nums,layer_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:1.0056896209716797\n",
      "layers:1, std:0.9921356439590454\n",
      "layers:2, std:1.0027211904525757\n",
      "layers:3, std:0.9852660298347473\n",
      "layers:4, std:0.9625363349914551\n",
      "layers:5, std:0.9498919248580933\n",
      "layers:6, std:0.9279433488845825\n",
      "layers:7, std:0.9370994567871094\n",
      "layers:8, std:0.9171197414398193\n",
      "layers:9, std:0.9329935908317566\n",
      "layers:10, std:0.9441112875938416\n",
      "layers:11, std:0.9663146138191223\n",
      "layers:12, std:0.9664993286132812\n",
      "layers:13, std:0.954406201839447\n",
      "layers:14, std:0.9583677053451538\n",
      "layers:15, std:0.9696717262268066\n",
      "layers:16, std:0.9842697978019714\n",
      "layers:17, std:0.9867438673973083\n",
      "layers:18, std:0.961632251739502\n",
      "layers:19, std:0.9636610746383667\n",
      "layers:20, std:0.9645311832427979\n",
      "layers:21, std:0.9377244114875793\n",
      "layers:22, std:0.9541034698486328\n",
      "layers:23, std:0.9769448637962341\n",
      "layers:24, std:0.96211177110672\n",
      "layers:25, std:0.9813463091850281\n",
      "layers:26, std:0.9384353756904602\n",
      "layers:27, std:0.9408388137817383\n",
      "layers:28, std:0.9674381613731384\n",
      "layers:29, std:0.9539881944656372\n",
      "layers:30, std:0.9651545882225037\n",
      "layers:31, std:0.9533388018608093\n",
      "layers:32, std:1.0098319053649902\n",
      "layers:33, std:1.038091778755188\n",
      "layers:34, std:1.032616138458252\n",
      "layers:35, std:1.0020833015441895\n",
      "layers:36, std:1.0187931060791016\n",
      "layers:37, std:1.012939691543579\n",
      "layers:38, std:1.0328857898712158\n",
      "layers:39, std:1.0683996677398682\n",
      "layers:40, std:1.109830379486084\n",
      "layers:41, std:1.0924675464630127\n",
      "layers:42, std:1.0938084125518799\n",
      "layers:43, std:1.055407166481018\n",
      "layers:44, std:1.0786770582199097\n",
      "layers:45, std:1.0332766771316528\n",
      "layers:46, std:1.0207645893096924\n",
      "layers:47, std:1.0128509998321533\n",
      "layers:48, std:1.046839952468872\n",
      "layers:49, std:1.0637340545654297\n",
      "layers:50, std:1.060847520828247\n",
      "layers:51, std:1.0405330657958984\n",
      "layers:52, std:1.045189380645752\n",
      "layers:53, std:0.9875563383102417\n",
      "layers:54, std:0.9475107789039612\n",
      "layers:55, std:0.9670119881629944\n",
      "layers:56, std:0.989435613155365\n",
      "layers:57, std:0.987568199634552\n",
      "layers:58, std:1.0040640830993652\n",
      "layers:59, std:1.0274125337600708\n",
      "layers:60, std:1.010284185409546\n",
      "layers:61, std:0.9697642922401428\n",
      "layers:62, std:0.9706739187240601\n",
      "layers:63, std:0.9959947466850281\n",
      "layers:64, std:1.0165536403656006\n",
      "layers:65, std:1.0514367818832397\n",
      "layers:66, std:1.025294542312622\n",
      "layers:67, std:1.0274465084075928\n",
      "layers:68, std:1.0650111436843872\n",
      "layers:69, std:1.080263376235962\n",
      "layers:70, std:1.0990101099014282\n",
      "layers:71, std:1.1004756689071655\n",
      "layers:72, std:1.1118474006652832\n",
      "layers:73, std:1.0605359077453613\n",
      "layers:74, std:0.9797796010971069\n",
      "layers:75, std:0.9993115067481995\n",
      "layers:76, std:1.035996437072754\n",
      "layers:77, std:1.0664199590682983\n",
      "layers:78, std:1.0379595756530762\n",
      "layers:79, std:1.034385085105896\n",
      "layers:80, std:1.0517971515655518\n",
      "layers:81, std:1.0488488674163818\n",
      "layers:82, std:1.0293992757797241\n",
      "layers:83, std:1.0288344621658325\n",
      "layers:84, std:1.015823245048523\n",
      "layers:85, std:0.9895011186599731\n",
      "layers:86, std:0.9652419686317444\n",
      "layers:87, std:0.9868187308311462\n",
      "layers:88, std:0.9824445247650146\n",
      "layers:89, std:1.0099537372589111\n",
      "layers:90, std:0.970339834690094\n",
      "layers:91, std:0.9217690825462341\n",
      "layers:92, std:0.9286347031593323\n",
      "layers:93, std:0.8622499704360962\n",
      "layers:94, std:0.8570361733436584\n",
      "layers:95, std:0.8332106471061707\n",
      "layers:96, std:0.8439131379127502\n",
      "layers:97, std:0.8293693661689758\n",
      "layers:98, std:0.8282068967819214\n",
      "layers:99, std:0.8343551754951477\n"
     ]
    }
   ],
   "source": [
    "output = net3(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2775, -0.5735, -1.9407,  ...,  1.6885,  0.9244, -0.4549],\n",
       "        [ 1.4726, -0.3046, -1.1130,  ...,  0.3368,  0.3650,  0.5948],\n",
       "        [ 0.7473, -0.3004,  0.2890,  ...,  0.8805,  1.0801,  0.2106],\n",
       "        ...,\n",
       "        [ 1.2075,  0.0306, -0.1541,  ..., -2.4691, -1.0989,  1.5947],\n",
       "        [ 0.7168,  0.3330, -0.6735,  ...,  0.0168,  0.4514, -0.5064],\n",
       "        [-0.9210, -0.5308,  1.4379,  ..., -0.5287, -0.4071, -0.3429]],\n",
       "       grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看梯度消失的例子  \n",
    "我们在线性层后面加上激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,neural_num,layers):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear = nn.ModuleList([nn.Linear(neural_num,neural_num,bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "    def forward(self,x):\n",
    "        for (i,linear) in enumerate(self.linear):\n",
    "            x =linear(x)\n",
    "            x = torch.tanh(x)\n",
    "            print(\"layers:{}, std:{}\".format(i,x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "        return x\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.normal_(m.weight.data,std=np.sqrt(1/self.neural_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_ = MLP(neurAL_nums,layer_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:0.6284132599830627\n",
      "layers:1, std:0.4860682189464569\n",
      "layers:2, std:0.4059627950191498\n",
      "layers:3, std:0.35384225845336914\n",
      "layers:4, std:0.3184562027454376\n",
      "layers:5, std:0.29324835538864136\n",
      "layers:6, std:0.27135857939720154\n",
      "layers:7, std:0.2467942088842392\n",
      "layers:8, std:0.23449578881263733\n",
      "layers:9, std:0.21716250479221344\n",
      "layers:10, std:0.2077142596244812\n",
      "layers:11, std:0.2040049433708191\n",
      "layers:12, std:0.20062442123889923\n",
      "layers:13, std:0.1967809498310089\n",
      "layers:14, std:0.1906975954771042\n",
      "layers:15, std:0.18707753717899323\n",
      "layers:16, std:0.18422092497348785\n",
      "layers:17, std:0.17498335242271423\n",
      "layers:18, std:0.17414061725139618\n",
      "layers:19, std:0.1663168966770172\n",
      "layers:20, std:0.15726089477539062\n",
      "layers:21, std:0.15164928138256073\n",
      "layers:22, std:0.14775006473064423\n",
      "layers:23, std:0.14356309175491333\n",
      "layers:24, std:0.14003361761569977\n",
      "layers:25, std:0.13919799029827118\n",
      "layers:26, std:0.1360105574131012\n",
      "layers:27, std:0.1325470507144928\n",
      "layers:28, std:0.13525713980197906\n",
      "layers:29, std:0.13077303767204285\n",
      "layers:30, std:0.13200269639492035\n",
      "layers:31, std:0.12744981050491333\n",
      "layers:32, std:0.1260465383529663\n",
      "layers:33, std:0.12320699542760849\n",
      "layers:34, std:0.12337420880794525\n",
      "layers:35, std:0.11703339219093323\n",
      "layers:36, std:0.11457812786102295\n",
      "layers:37, std:0.11446121335029602\n",
      "layers:38, std:0.11105963587760925\n",
      "layers:39, std:0.11022739112377167\n",
      "layers:40, std:0.10933700203895569\n",
      "layers:41, std:0.10887087136507034\n",
      "layers:42, std:0.10837073624134064\n",
      "layers:43, std:0.10751370340585709\n",
      "layers:44, std:0.10960479080677032\n",
      "layers:45, std:0.11011943221092224\n",
      "layers:46, std:0.10832859575748444\n",
      "layers:47, std:0.10523578524589539\n",
      "layers:48, std:0.10573968291282654\n",
      "layers:49, std:0.10263372212648392\n",
      "layers:50, std:0.10340895503759384\n",
      "layers:51, std:0.09910500794649124\n",
      "layers:52, std:0.09858996421098709\n",
      "layers:53, std:0.09475591778755188\n",
      "layers:54, std:0.09471659362316132\n",
      "layers:55, std:0.0943455845117569\n",
      "layers:56, std:0.0951632484793663\n",
      "layers:57, std:0.09531685709953308\n",
      "layers:58, std:0.09673074632883072\n",
      "layers:59, std:0.0972248837351799\n",
      "layers:60, std:0.09836578369140625\n",
      "layers:61, std:0.09560240805149078\n",
      "layers:62, std:0.09766887128353119\n",
      "layers:63, std:0.09858464449644089\n",
      "layers:64, std:0.0953419879078865\n",
      "layers:65, std:0.09224823117256165\n",
      "layers:66, std:0.09151486307382584\n",
      "layers:67, std:0.09138338267803192\n",
      "layers:68, std:0.09101301431655884\n",
      "layers:69, std:0.0882829800248146\n",
      "layers:70, std:0.09174918383359909\n",
      "layers:71, std:0.0895889401435852\n",
      "layers:72, std:0.0894000381231308\n",
      "layers:73, std:0.08933126926422119\n",
      "layers:74, std:0.09099489450454712\n",
      "layers:75, std:0.09016191959381104\n",
      "layers:76, std:0.09353615343570709\n",
      "layers:77, std:0.09497542679309845\n",
      "layers:78, std:0.09396465867757797\n",
      "layers:79, std:0.09607908874750137\n",
      "layers:80, std:0.09840577095746994\n",
      "layers:81, std:0.09493815898895264\n",
      "layers:82, std:0.09379373490810394\n",
      "layers:83, std:0.09369900077581406\n",
      "layers:84, std:0.0948004350066185\n",
      "layers:85, std:0.09508491307497025\n",
      "layers:86, std:0.09680397808551788\n",
      "layers:87, std:0.0997321605682373\n",
      "layers:88, std:0.09949829429388046\n",
      "layers:89, std:0.09136656671762466\n",
      "layers:90, std:0.09226755052804947\n",
      "layers:91, std:0.09200505912303925\n",
      "layers:92, std:0.09107020497322083\n",
      "layers:93, std:0.0887598842382431\n",
      "layers:94, std:0.08589506149291992\n",
      "layers:95, std:0.08343054354190826\n",
      "layers:96, std:0.08215973526239395\n",
      "layers:97, std:0.08225473761558533\n",
      "layers:98, std:0.08319354057312012\n",
      "layers:99, std:0.08389632403850555\n"
     ]
    }
   ],
   "source": [
    "output = net_(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到方差越来越小，那么权重数据也会越来越小，最终会导致梯度的消失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的初始化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xavier初始化\n",
    "方差一致性：保持数据尺度维持在适当范围，通常方差为1  \n",
    "激活函数：饱和函数，如sigmoid,Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$n_i*D(w) = 1$$  \n",
    "$$n_{i+1}*D(w) = 1$$   \n",
    "权重的方差，输入层神经元个数$$n_i$$  \n",
    "$$n_{i+1}$$输出层神经元个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=》$$D(w)=2/n_i+n_{i+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,neural_num,layers):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear = nn.ModuleList([nn.Linear(neural_num,neural_num,bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "    def forward(self,x):\n",
    "        for (i,linear) in enumerate(self.linear):\n",
    "            x =linear(x)\n",
    "            x = torch.tanh(x)\n",
    "            print(\"layers:{}, std:{}\".format(i,x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "        return x\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                a = np.sqrt(6/(self.neural_num+self.neural_num))\n",
    "                tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                a*=tanh_gain\n",
    "                nn.init.uniform(m.weight.data,-a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_xver = MLP(neurAL_nums,layer_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanggong-study/anaconda3/envs/py_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
     ]
    }
   ],
   "source": [
    "net_xver.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:0.7615704536437988\n",
      "layers:1, std:0.7009486556053162\n",
      "layers:2, std:0.6695532202720642\n",
      "layers:3, std:0.6599892973899841\n",
      "layers:4, std:0.6508842706680298\n",
      "layers:5, std:0.6599737405776978\n",
      "layers:6, std:0.6529982089996338\n",
      "layers:7, std:0.6635789275169373\n",
      "layers:8, std:0.6567519307136536\n",
      "layers:9, std:0.6483654975891113\n",
      "layers:10, std:0.6481558680534363\n",
      "layers:11, std:0.6499091386795044\n",
      "layers:12, std:0.6511178612709045\n",
      "layers:13, std:0.6461487412452698\n",
      "layers:14, std:0.6440340876579285\n",
      "layers:15, std:0.651336669921875\n",
      "layers:16, std:0.656968355178833\n",
      "layers:17, std:0.6554034948348999\n",
      "layers:18, std:0.6466189026832581\n",
      "layers:19, std:0.6502505540847778\n",
      "layers:20, std:0.6422507166862488\n",
      "layers:21, std:0.6446830630302429\n",
      "layers:22, std:0.6496829390525818\n",
      "layers:23, std:0.6572242975234985\n",
      "layers:24, std:0.6441609263420105\n",
      "layers:25, std:0.641246497631073\n",
      "layers:26, std:0.6543492674827576\n",
      "layers:27, std:0.6472375392913818\n",
      "layers:28, std:0.6486672163009644\n",
      "layers:29, std:0.6516335010528564\n",
      "layers:30, std:0.652384340763092\n",
      "layers:31, std:0.6563328504562378\n",
      "layers:32, std:0.659392774105072\n",
      "layers:33, std:0.6580159068107605\n",
      "layers:34, std:0.6604445576667786\n",
      "layers:35, std:0.658670961856842\n",
      "layers:36, std:0.6508934497833252\n",
      "layers:37, std:0.6443811058998108\n",
      "layers:38, std:0.6439722180366516\n",
      "layers:39, std:0.6524071097373962\n",
      "layers:40, std:0.6508054733276367\n",
      "layers:41, std:0.6518692374229431\n",
      "layers:42, std:0.6563901305198669\n",
      "layers:43, std:0.6549925208091736\n",
      "layers:44, std:0.6473967432975769\n",
      "layers:45, std:0.6510197520256042\n",
      "layers:46, std:0.6396005153656006\n",
      "layers:47, std:0.6461582779884338\n",
      "layers:48, std:0.6520621180534363\n",
      "layers:49, std:0.6421274542808533\n",
      "layers:50, std:0.6434420347213745\n",
      "layers:51, std:0.650816023349762\n",
      "layers:52, std:0.6498100757598877\n",
      "layers:53, std:0.6500130295753479\n",
      "layers:54, std:0.6468944549560547\n",
      "layers:55, std:0.6402463912963867\n",
      "layers:56, std:0.648018479347229\n",
      "layers:57, std:0.6529120802879333\n",
      "layers:58, std:0.6597766876220703\n",
      "layers:59, std:0.6508221626281738\n",
      "layers:60, std:0.6584094166755676\n",
      "layers:61, std:0.6456770300865173\n",
      "layers:62, std:0.6396130919456482\n",
      "layers:63, std:0.6479036808013916\n",
      "layers:64, std:0.6532855033874512\n",
      "layers:65, std:0.6606718301773071\n",
      "layers:66, std:0.6515478491783142\n",
      "layers:67, std:0.6578220129013062\n",
      "layers:68, std:0.6586560606956482\n",
      "layers:69, std:0.6571361422538757\n",
      "layers:70, std:0.6619457006454468\n",
      "layers:71, std:0.655978262424469\n",
      "layers:72, std:0.6533045172691345\n",
      "layers:73, std:0.6537389159202576\n",
      "layers:74, std:0.6442966461181641\n",
      "layers:75, std:0.651023268699646\n",
      "layers:76, std:0.6509091258049011\n",
      "layers:77, std:0.6546738743782043\n",
      "layers:78, std:0.6530566215515137\n",
      "layers:79, std:0.6511789560317993\n",
      "layers:80, std:0.6421435475349426\n",
      "layers:81, std:0.6429662704467773\n",
      "layers:82, std:0.6483616232872009\n",
      "layers:83, std:0.6449397206306458\n",
      "layers:84, std:0.647833526134491\n",
      "layers:85, std:0.6440481543540955\n",
      "layers:86, std:0.6442257761955261\n",
      "layers:87, std:0.6435502171516418\n",
      "layers:88, std:0.6484062075614929\n",
      "layers:89, std:0.6541556119918823\n",
      "layers:90, std:0.6537227034568787\n",
      "layers:91, std:0.6520593762397766\n",
      "layers:92, std:0.6543694734573364\n",
      "layers:93, std:0.6560428738594055\n",
      "layers:94, std:0.65505051612854\n",
      "layers:95, std:0.6594477891921997\n",
      "layers:96, std:0.6453339457511902\n",
      "layers:97, std:0.6412146091461182\n",
      "layers:98, std:0.6475186347961426\n",
      "layers:99, std:0.653559684753418\n"
     ]
    }
   ],
   "source": [
    "output = net_xver(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现经过了初始化后权重值分布在0.64-0.65左右  \n",
    "pytorch里内置函数有相关的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,neural_num,layers):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear = nn.ModuleList([nn.Linear(neural_num,neural_num,bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "    def forward(self,x):\n",
    "        for (i,linear) in enumerate(self.linear):\n",
    "            x =linear(x)\n",
    "            x= torch.tanh(x)\n",
    "            print(\"layers:{}, std:{}\".format(i,x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "        return x\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                nn.init.xavier_uniform(m.weight.data,gain=tanh_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:0.7612550854682922\n",
      "layers:1, std:0.6903542280197144\n",
      "layers:2, std:0.662228524684906\n",
      "layers:3, std:0.6574481129646301\n",
      "layers:4, std:0.648288905620575\n",
      "layers:5, std:0.6484530568122864\n",
      "layers:6, std:0.6495718359947205\n",
      "layers:7, std:0.6528733372688293\n",
      "layers:8, std:0.6482165455818176\n",
      "layers:9, std:0.6567924618721008\n",
      "layers:10, std:0.6460853219032288\n",
      "layers:11, std:0.6438324451446533\n",
      "layers:12, std:0.6438490748405457\n",
      "layers:13, std:0.6451663374900818\n",
      "layers:14, std:0.6498647928237915\n",
      "layers:15, std:0.6491470336914062\n",
      "layers:16, std:0.6414947509765625\n",
      "layers:17, std:0.6499527096748352\n",
      "layers:18, std:0.6527339220046997\n",
      "layers:19, std:0.6521665453910828\n",
      "layers:20, std:0.6528139114379883\n",
      "layers:21, std:0.6483110785484314\n",
      "layers:22, std:0.6438817381858826\n",
      "layers:23, std:0.6520076990127563\n",
      "layers:24, std:0.6498242020606995\n",
      "layers:25, std:0.6567388772964478\n",
      "layers:26, std:0.6621300578117371\n",
      "layers:27, std:0.6573572754859924\n",
      "layers:28, std:0.6543906331062317\n",
      "layers:29, std:0.6503568887710571\n",
      "layers:30, std:0.6470530033111572\n",
      "layers:31, std:0.6576794981956482\n",
      "layers:32, std:0.653170108795166\n",
      "layers:33, std:0.6629514694213867\n",
      "layers:34, std:0.655789315700531\n",
      "layers:35, std:0.6522977948188782\n",
      "layers:36, std:0.6478632688522339\n",
      "layers:37, std:0.6541997790336609\n",
      "layers:38, std:0.6595317721366882\n",
      "layers:39, std:0.6579177379608154\n",
      "layers:40, std:0.6566987037658691\n",
      "layers:41, std:0.6565778851509094\n",
      "layers:42, std:0.6564901471138\n",
      "layers:43, std:0.6601676344871521\n",
      "layers:44, std:0.665149986743927\n",
      "layers:45, std:0.6588437557220459\n",
      "layers:46, std:0.6650771498680115\n",
      "layers:47, std:0.652458667755127\n",
      "layers:48, std:0.6546816229820251\n",
      "layers:49, std:0.6437140703201294\n",
      "layers:50, std:0.6463153958320618\n",
      "layers:51, std:0.6480663418769836\n",
      "layers:52, std:0.6477883458137512\n",
      "layers:53, std:0.6463467478752136\n",
      "layers:54, std:0.6561120748519897\n",
      "layers:55, std:0.6652715802192688\n",
      "layers:56, std:0.6572922468185425\n",
      "layers:57, std:0.6585249304771423\n",
      "layers:58, std:0.6556077599525452\n",
      "layers:59, std:0.6507279276847839\n",
      "layers:60, std:0.6514453291893005\n",
      "layers:61, std:0.6518751978874207\n",
      "layers:62, std:0.6632979512214661\n",
      "layers:63, std:0.6538383960723877\n",
      "layers:64, std:0.6487346887588501\n",
      "layers:65, std:0.6494333744049072\n",
      "layers:66, std:0.6532903909683228\n",
      "layers:67, std:0.6533477306365967\n",
      "layers:68, std:0.6567427515983582\n",
      "layers:69, std:0.654830813407898\n",
      "layers:70, std:0.6465599536895752\n",
      "layers:71, std:0.651902437210083\n",
      "layers:72, std:0.6544078588485718\n",
      "layers:73, std:0.6492218375205994\n",
      "layers:74, std:0.6530242562294006\n",
      "layers:75, std:0.6527369618415833\n",
      "layers:76, std:0.6511095762252808\n",
      "layers:77, std:0.6572321653366089\n",
      "layers:78, std:0.653693437576294\n",
      "layers:79, std:0.6550726294517517\n",
      "layers:80, std:0.6511086821556091\n",
      "layers:81, std:0.6498749852180481\n",
      "layers:82, std:0.6470234394073486\n",
      "layers:83, std:0.6448495984077454\n",
      "layers:84, std:0.6503244042396545\n",
      "layers:85, std:0.648643970489502\n",
      "layers:86, std:0.6548869013786316\n",
      "layers:87, std:0.6516827940940857\n",
      "layers:88, std:0.6529231071472168\n",
      "layers:89, std:0.6519263982772827\n",
      "layers:90, std:0.6487756967544556\n",
      "layers:91, std:0.6528890132904053\n",
      "layers:92, std:0.6538798213005066\n",
      "layers:93, std:0.6495431661605835\n",
      "layers:94, std:0.6509060263633728\n",
      "layers:95, std:0.6592230796813965\n",
      "layers:96, std:0.6522994637489319\n",
      "layers:97, std:0.659611701965332\n",
      "layers:98, std:0.6598445177078247\n",
      "layers:99, std:0.657442569732666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanggong-study/anaconda3/envs/py_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
     ]
    }
   ],
   "source": [
    "net_xver1= MLP(neurAL_nums,layer_nums)\n",
    "net_xver.initialize()\n",
    "output = net_xver(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种初始化方法不适用relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaiming初始化\n",
    "* 方差一致性：保持数据尺度维持在恰当范围，通常方差为1\n",
    "* 机会函数：ReLU，及其变种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self,neural_num,layers):\n",
    "        super(MLP2,self).__init__()\n",
    "        self.linear = nn.ModuleList([nn.Linear(neural_num,neural_num,bias=False) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "    def forward(self,x):\n",
    "        for (i,linear) in enumerate(self.linear):\n",
    "            x =linear(x)\n",
    "            x= torch.relu(x)\n",
    "            print(\"layers:{}, std:{}\".format(i,x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "        return x\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.kaiming_normal(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanggong-study/anaconda3/envs/py_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:0.7595686912536621\n",
      "layers:1, std:0.6903108954429626\n",
      "layers:2, std:0.6655719876289368\n",
      "layers:3, std:0.6573150753974915\n",
      "layers:4, std:0.656446635723114\n",
      "layers:5, std:0.6504065990447998\n",
      "layers:6, std:0.6504220366477966\n",
      "layers:7, std:0.6609078049659729\n",
      "layers:8, std:0.6630220413208008\n",
      "layers:9, std:0.6533516645431519\n",
      "layers:10, std:0.6591590642929077\n",
      "layers:11, std:0.6552177667617798\n",
      "layers:12, std:0.6570743322372437\n",
      "layers:13, std:0.6541009545326233\n",
      "layers:14, std:0.6521506905555725\n",
      "layers:15, std:0.6581734418869019\n",
      "layers:16, std:0.6517672538757324\n",
      "layers:17, std:0.640683650970459\n",
      "layers:18, std:0.6404054164886475\n",
      "layers:19, std:0.6370103359222412\n",
      "layers:20, std:0.6513935923576355\n",
      "layers:21, std:0.6522902250289917\n",
      "layers:22, std:0.6489609479904175\n",
      "layers:23, std:0.6537424325942993\n",
      "layers:24, std:0.6475028991699219\n",
      "layers:25, std:0.6530891060829163\n",
      "layers:26, std:0.6552497744560242\n",
      "layers:27, std:0.652744710445404\n",
      "layers:28, std:0.648811936378479\n",
      "layers:29, std:0.6410679221153259\n",
      "layers:30, std:0.6488082408905029\n",
      "layers:31, std:0.6498005390167236\n",
      "layers:32, std:0.6530632972717285\n",
      "layers:33, std:0.6554574370384216\n",
      "layers:34, std:0.6550918817520142\n",
      "layers:35, std:0.6513184309005737\n",
      "layers:36, std:0.6581239700317383\n",
      "layers:37, std:0.6552282571792603\n",
      "layers:38, std:0.6512407660484314\n",
      "layers:39, std:0.6523638367652893\n",
      "layers:40, std:0.6534111499786377\n",
      "layers:41, std:0.6555134654045105\n",
      "layers:42, std:0.6501386761665344\n",
      "layers:43, std:0.6428613066673279\n",
      "layers:44, std:0.639997124671936\n",
      "layers:45, std:0.645381510257721\n",
      "layers:46, std:0.6454491019248962\n",
      "layers:47, std:0.6536351442337036\n",
      "layers:48, std:0.653618574142456\n",
      "layers:49, std:0.6520172953605652\n",
      "layers:50, std:0.6446619629859924\n",
      "layers:51, std:0.6472935080528259\n",
      "layers:52, std:0.6508845686912537\n",
      "layers:53, std:0.6545895338058472\n",
      "layers:54, std:0.6535233855247498\n",
      "layers:55, std:0.6528732180595398\n",
      "layers:56, std:0.655658483505249\n",
      "layers:57, std:0.6498314142227173\n",
      "layers:58, std:0.6477911472320557\n",
      "layers:59, std:0.6520750522613525\n",
      "layers:60, std:0.6460719108581543\n",
      "layers:61, std:0.6577399969100952\n",
      "layers:62, std:0.6541648507118225\n",
      "layers:63, std:0.6501820683479309\n",
      "layers:64, std:0.6509562134742737\n",
      "layers:65, std:0.6549961566925049\n",
      "layers:66, std:0.6532755494117737\n",
      "layers:67, std:0.6492959856987\n",
      "layers:68, std:0.6523653268814087\n",
      "layers:69, std:0.6483544111251831\n",
      "layers:70, std:0.6454943418502808\n",
      "layers:71, std:0.6508254408836365\n",
      "layers:72, std:0.6532219052314758\n",
      "layers:73, std:0.6540840268135071\n",
      "layers:74, std:0.6512205600738525\n",
      "layers:75, std:0.6539897918701172\n",
      "layers:76, std:0.6507927179336548\n",
      "layers:77, std:0.6428577899932861\n",
      "layers:78, std:0.6485974788665771\n",
      "layers:79, std:0.6458570957183838\n",
      "layers:80, std:0.6487144231796265\n",
      "layers:81, std:0.6533914804458618\n",
      "layers:82, std:0.6535752415657043\n",
      "layers:83, std:0.6555585861206055\n",
      "layers:84, std:0.6528090834617615\n",
      "layers:85, std:0.654218316078186\n",
      "layers:86, std:0.6511552333831787\n",
      "layers:87, std:0.6518172025680542\n",
      "layers:88, std:0.6504950523376465\n",
      "layers:89, std:0.6482343077659607\n",
      "layers:90, std:0.651085615158081\n",
      "layers:91, std:0.6499447822570801\n",
      "layers:92, std:0.6554445028305054\n",
      "layers:93, std:0.6522966027259827\n",
      "layers:94, std:0.6545044183731079\n",
      "layers:95, std:0.6606632471084595\n",
      "layers:96, std:0.6529313921928406\n",
      "layers:97, std:0.6551584005355835\n",
      "layers:98, std:0.653554379940033\n",
      "layers:99, std:0.6541115045547485\n"
     ]
    }
   ],
   "source": [
    "net_xver1= MLP2(neurAL_nums,layer_nums)\n",
    "net_xver1.initialize()\n",
    "output = net_xver(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "十种初始化方法\n",
    "* Xavier均匀分布\n",
    "* Xavier标准正态分布\n",
    "* Kaiming均匀分布\n",
    "* Kaiming标准正态分布\n",
    "* 均匀分布\n",
    "* 正态分布\n",
    "* 常数分布\n",
    "* 正交矩阵初始化\n",
    "* 单位矩阵初始化\n",
    "* 稀疏矩阵初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.init.calculate_gain()的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = x.std()/out.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_gain = nn.init.calculate_gain('tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gain:1.5971213579177856,tan_gain1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('gain:{},tan_gain{}'.format(gain,tanh_gain))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "py_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
